

## 复习MCTS

[蒙特卡洛树搜索最通俗入门指南 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/53948964)

- 为什么minimax对围棋效果差？
  - 棋局评判能力要求更高
    - 围棋的棋子之间关系复杂，
    - 国际象棋的棋局特征很明显
  - 计算能力要求太高
    - 围棋19x19, 象棋8x8
- **蒙特卡洛方法** (不是蒙特卡洛树搜索)
  - 随机继续下棋10000次计算胜率
    - 伪算法，MCTS才是真算法
    - 需要下很多次

### MCTS基本概念

> 根据「棋感」在脑海里大致筛选出了几种「最可能」的走法，然后再想走了这几种走法之后对手「最可能」的走法，然后再想自己接下来「最可能」的走法。这其实就是 MCTS 算法的设计思路。 

意思是剪枝的minimax吗？

它经历下面 3 个过程（重复千千万万次）：

1. **选择**（Selection）
2. **扩展** (expansion)
3. **模拟**（Simulation）
4. **回溯**（Backpropagation）

### 选择

博弈树和minimax是一模一样的，只是搜索方法不同。

博弈树的节点（局面）有三种，

- 未访问

- 未完全展开
  - 子节点有很多个，其中一部分快速走子过

- 完全展开
  - 子节点全部快速走子过。

从未访问中，根据UCT值选择一个节点，对这个节点进行扩展。

倾向于选择没被统计过的节点。

### 扩展

刚才选择的节点是没有统计信息的节点，给他初始化统计信息0/0，对它进行模拟。

只下一局？

### 模拟

我们每个节点（每个节点代表每个不同的局面）都有两个值，代表这个节点**以及它的子节点**模拟的次数和赢的次数，比如模拟了 10 次，赢了 4 盘，记为 4/10。

### 反向传播

模拟完的节点的父节点要进行更新。

![img](https://pic1.zhimg.com/80/v2-b958662a0be8daf52bea1cd735a7575c_720w.jpg)

### Alpha Go的理解

- softmax 分类网络，根据贪心策略快速走棋。
- 估值网络：13层卷积神经网络位置评估，
- Alpha Zero直接不模拟：19层CNN评估节点的胜率，然后作为模拟结果？



### 我还是没理解

![img](https://pic4.zhimg.com/80/v2-16cafcfda07f07733d2a2326500b6bd7_720w.jpg)

1. 该节点所有可行动作都已经被拓展过
2. 该节点有可行动作还未被拓展过
3. 这个节点游戏已经结束了(例如已经连成五子的五子棋局面)

这个分类是对的

如果所有动作都扩展过，那么UCB选择一个最好的子节点，进一步对这个子节点分析

如果没有扩展过，就先扩展还没扩展的动作（被扩展的节点还是我们现在这个节点）

如果是终局，执行反向传播



刚才那个节点要被扩展，或者它的子节点要扩展

那就  模拟 这个 动作后的棋局



UCT是置信度，比如3次抛硬币都是正面朝上，那么求出硬币正面朝上的置信度，与统计次数是有关的

让探索次数少的节点有更多机会



## Alpha Zero的理解

[最强通用棋类AI，AlphaZero强化学习算法解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/325865136)

这一类计算机系统由以下三个组件构成：

\1. 人为定义的[评价函数](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Evaluation_function)；

\2. 博弈树搜索算法；

\3. 极为强悍的硬件设备。

回想一下，像DeepBlue那样依赖于人为定义的“[评价函数](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Evaluation_function)”的系统会把棋盘的盘面状态作为输入，再输出该状态的“价值”。

> 就像许多其他棋盘类游戏一样，最佳的走法可能需要牺牲短期利益来换取长期利益。在 Dots and Boxes 游戏中，有时最好不要急于得分并获得额外先手，相反，要迫使对手走某一步棋。因此，我们必须考虑大量复杂场景并精心调制评价函数！

>  击败Kasparov的[评价函数](https://link.zhihu.com/?target=https%3A//www.sciencedirect.com/science/article/pii/S0004370201001291/pdf)需要识别多达8000个盘面特征！而且其中绝大多数都是手动描述并调整的！

如今，对于深度学习模型来说，输入一张照片然后识别出照片里是猫还是狗简直简单到爆了。那么有个想法就是，把棋盘盘面作为一个深度学习模型的输入并且训练它，让它预测这样的盘面布置是会输还是会赢。

但是，要训练一个机器学习模型，就需要数据，海量的数据。从哪儿能得到那么多棋局博弈的数据呢？很简单，我们就让电脑自己跟自己下着玩儿，生成一堆棋局，然后再把它们做成一个数据集用来训练。

### **AlphaZero的训练算法**

这个算法简单明了：

\1. 让计算机自我博弈数局，记录每一步走棋。一旦胜负已分，就给之前的每一步走棋打上标签——棋面最终是“赢”或是“输”。如此一来，我们就获得了一个可以用于神经网络（Neural Network，NN）训练的数据集，让该网络学会判断给定棋面是“赢面”还是“输面”；

\2. 复制这个神经网络。用上一步得到的数据集训练该克隆网络；

\3. 让克隆网络与原始神经网络互相博弈；

\4. 上一步中获胜的网络留下，败者弃之；

\5. 重复第1步。

### **AlphaZero的组件**

AlphaZero由两部分构成。我们已经提及了第一部分，就是神经网络。第二部分则是“蒙特卡洛树搜索（Monte Carlo Tree Search）”，或者简称MCTS。

\1. **神经网络（NN）**。以棋面作为输入，输出该棋面的“价值”，外加所有可能走法的概率分布。

\2. **蒙特卡洛树搜索（MCTS）**。**理想情况下，使用神经网络就足以选择下一步走法了**。不过，我们仍然希望考虑**尽可能多的棋面**，并确保我们的的确确选择了最好的走法。MTCS和Minimax一样，是一种可以帮助我们寻找可能棋面的算法。与Minimax不同的是，**MTCS能够帮助我们更加高效地搜寻博弈树**。

### **让我们深入细节，看一看下一步走棋究竟是如何得到的**